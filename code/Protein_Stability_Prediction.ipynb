{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFmiiap05Z5b"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oPbtZoncrZh",
        "outputId": "d0b5d573-3703-4ec3-92eb-62d6d77a3b92"
      },
      "outputs": [],
      "source": [
        "# !pip install py3Dmol\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npEmFDu117HD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import requests # for pulling info from urls\n",
        "import Bio.PDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6epoKhgmn2i",
        "outputId": "8977fdd5-ce90-4e10-9002-8813aadc29bf"
      },
      "outputs": [],
      "source": [
        "# mount drive (if using colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOL1m5LXFkfG"
      },
      "outputs": [],
      "source": [
        "# set data dir\n",
        "DATA_DIR = \"data\"\n",
        "assert os.path.exists(DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSgzzHswFbXC"
      },
      "source": [
        "# Stability Data Wrangling\n",
        "NOTE: These cells do not need to be run again if you already have the cleaned/combined data stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQYroa15mMc"
      },
      "source": [
        "### Data cleanup\n",
        "Load data for ThermomutDB and FireProtDB and remove any entries that have missing values for critical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o44kFZB1-lz"
      },
      "outputs": [],
      "source": [
        "FIREPROT_PTH = os.path.join(DATA_DIR, \"fireprotdb_results.csv\")\n",
        "fireprot_df = pd.read_csv(FIREPROT_PTH)\n",
        "\n",
        "THERMOMUT_PTH = os.path.join(DATA_DIR, \"thermomutdb.json\")\n",
        "thermomut_file = open(THERMOMUT_PTH)\n",
        "thermomut_df = json.load(thermomut_file)\n",
        "thermomut_df = pd.DataFrame(thermomut_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLYgHlEFx18v"
      },
      "outputs": [],
      "source": [
        "# remove entries that are not single-point mutation (fireprot already only stores only single-point)\n",
        "thermomut_df = thermomut_df[thermomut_df['mutation_type'] == 'Single']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-FWu87R_CLH"
      },
      "outputs": [],
      "source": [
        "def drop_nas(ess_cols, df: pd.DataFrame):\n",
        "  for ec in ess_cols:\n",
        "    df = df[df[ec].notna()]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e-ce0dM-V8c"
      },
      "outputs": [],
      "source": [
        "essential_fire_cols = [\"chain\", \"ddG\", \"pdb_id\", \"position\", \"interpro_families\", \"mutation\", \"uniprot_id\", \"wild_type\"]\n",
        "fireprot_df = drop_nas(essential_fire_cols, fireprot_df)\n",
        "\n",
        "essential_therm_cols = [\"mutated_chain\", \"ddg\", \"mutation_code\", \"PDB_wild\", \"uniprot\"]\n",
        "thermomut_df = drop_nas(essential_therm_cols, thermomut_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMfMfozvYt3z"
      },
      "outputs": [],
      "source": [
        "# these fireprot entries were found to have sequence entries inconsistent with UniProtDB\n",
        "fireprot_df = fireprot_df[fireprot_df['uniprot_id'] != \"P60174\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ6BSpfW_dvP"
      },
      "outputs": [],
      "source": [
        "# for some reason thermomut has 8 entries with \"-\" as the uniprot id\n",
        "# get rid of these\n",
        "uni_lens = thermomut_df['uniprot'].map(lambda s: len(s))\n",
        "thermomut_df = thermomut_df[uni_lens == 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs53uoqA9lCE"
      },
      "outputs": [],
      "source": [
        "# make interpro_families entries lists\n",
        "fireprot_df['interpro_families'] = fireprot_df['interpro_families'].map(lambda s: s.split(\"|\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zuff3eAZ01"
      },
      "outputs": [],
      "source": [
        "print(len(fireprot_df))\n",
        "print(len(thermomut_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCfpM4Nb6xP"
      },
      "source": [
        "### Extract Mutation Info from ThermoMutDB\n",
        "Thermomut stores mutation info as a 3-digit code xNy where x is the original amino acid, N is the position in the sequence where the mutation occurs, and y is the amino acid in the mutated sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "129cJsRQNkNk"
      },
      "outputs": [],
      "source": [
        "# remove erroneous mutation codes\n",
        "# (shouldn't have > 2 letters or < 1 digit)\n",
        "\n",
        "letter_cnt = thermomut_df['mutation_code'].map(lambda s: sum(c.isalpha() for c in s))\n",
        "digit_cnt = thermomut_df['mutation_code'].map(lambda s: sum(c.isdigit() for c in s))\n",
        "\n",
        "thermomut_df = thermomut_df[letter_cnt == 2]\n",
        "thermomut_df = thermomut_df[digit_cnt > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bdi-NTmdf3m"
      },
      "outputs": [],
      "source": [
        "thermomut_df['wild_type'] = thermomut_df['mutation_code'].map(lambda s: s[0])\n",
        "thermomut_df['position'] = thermomut_df['mutation_code'].map(lambda s: int(s[1:-1]))\n",
        "thermomut_df['mutant_type'] = thermomut_df['mutation_code'].map(lambda s:s[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG8R3pLMBlfk"
      },
      "outputs": [],
      "source": [
        "i = 500\n",
        "print(thermomut_df['mutation_code'].iloc[i])\n",
        "print(thermomut_df['wild_type'].iloc[i], thermomut_df['position'].iloc[i], thermomut_df['mutant_type'].iloc[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS_zTo5I90cJ"
      },
      "source": [
        "### Filling in Sequence Info for ThermoMutDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9hJwpHMsJla"
      },
      "outputs": [],
      "source": [
        "def get_seq(target_url: str):\n",
        "  response = requests.get(target_url)\n",
        "  data = response.text\n",
        "  # sequence starts on line 2\n",
        "  seq = data[data.index('\\n')+1:]\n",
        "  seq = \"\".join(seq.split())\n",
        "  return seq\n",
        "\n",
        "def get_thermomut_seqs(therm_df: pd.DataFrame, save_path: str):\n",
        "  thermomut_ids = set(therm_df['uniprot'])\n",
        "  # map uniprot id to sequence info\n",
        "  thermomut_seqs = {}\n",
        "  bad_entries = []\n",
        "  for id in tqdm(thermomut_ids):\n",
        "    url = \"https://rest.uniprot.org/uniprotkb/%s.fasta\" % id\n",
        "    try:\n",
        "      thermomut_seqs[id] = get_seq(url) \n",
        "    except ValueError:\n",
        "      # obsolete or deleted entries, will toss these out\n",
        "      bad_entries.append(id)\n",
        "\n",
        "  # obsolete: M5A5Y8, A0A410ZNC6\n",
        "  print(\"\\nSequences not found for:\", bad_entries)\n",
        "\n",
        "  # save sequence info obtained from uniprot\n",
        "  np.save(save_path, thermomut_seqs)\n",
        "  print(\"Saved thermomutdb sequence info to:\", save_path)\n",
        "  return thermomut_seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6tNmZpm6dw3"
      },
      "outputs": [],
      "source": [
        "def load_thermomut_seqs(save_file: str):\n",
        "  if not(os.path.exists(save_file)):\n",
        "    # have to load information from uniprot database, takes ~2-3 mins\n",
        "    print(\"Given file not found, loading seqs from uniprot...\")\n",
        "    return get_thermomut_seqs(thermomut_df, save_file)\n",
        "  thermomut_seqs = np.load(save_file, allow_pickle=True).item()\n",
        "  print(\"Loaded sequence info from '%s' successfully\" % save_file)\n",
        "  return thermomut_seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cHg2BEh8TZY"
      },
      "outputs": [],
      "source": [
        "# load sequence info\n",
        "save_file = os.path.join(DATA_DIR, \"thermomut_seqs.npy\")\n",
        "thermomut_seqs = load_thermomut_seqs(save_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvXj8A6U-r7Q"
      },
      "outputs": [],
      "source": [
        "# update thermomut_df base on loaded info\n",
        "thermomut_df['sequence'] = thermomut_df['uniprot'].map(thermomut_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnfXW7uwSNBb"
      },
      "outputs": [],
      "source": [
        "# validate that sequences were loaded properly by checking w/ overlap fireprot data\n",
        "thermomut_ids = set(thermomut_df['uniprot'])\n",
        "fireprot_ids = set(fireprot_df['uniprot_id'])\n",
        "overlap = thermomut_ids & fireprot_ids\n",
        "for id in overlap:\n",
        "  b = thermomut_df[thermomut_df['uniprot'] == id]['sequence'].iloc[0] == fireprot_df[fireprot_df['uniprot_id'] == id]['sequence'].iloc[0]\n",
        "  if not(b):\n",
        "    print(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2JyN9Xn0Ypq"
      },
      "source": [
        "### Removing nonsense mutations in databases\n",
        "Several entries have mutations where wild-type doesn't match position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApOifpUN0u8F"
      },
      "outputs": [],
      "source": [
        "# get rid of entries where wild-type doesn't match given info\n",
        "# see where mut-position is > sequence\n",
        "def filter(df: pd.DataFrame):\n",
        "  # remove any entries where position > sequence length\n",
        "  seq_lens = df['sequence'].map(lambda x: len(x))\n",
        "  positions = df['position']\n",
        "  filt_df = df[positions <= seq_lens]\n",
        "\n",
        "  # remove entries where position doesn't match wild-type\n",
        "  # this for some reason is mainly a thermomut problem\n",
        "  w_types = filt_df['wild_type']\n",
        "  positions = filt_df['position']\n",
        "\n",
        "  seqs = filt_df['sequence']\n",
        "  s_types = [x[p-1] for x, p in zip(seqs, positions)]\n",
        "  wrong_df = filt_df[w_types != s_types]\n",
        "  filt_df = filt_df[w_types == s_types]\n",
        "  return filt_df, wrong_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsQJrsOR06xD"
      },
      "outputs": [],
      "source": [
        "thermomut_df, excluded_therm_df = filter(thermomut_df)\n",
        "fireprot_df, excluded_fire_df = filter(fireprot_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09r4fn8LovvC"
      },
      "outputs": [],
      "source": [
        "print(len(excluded_therm_df))\n",
        "print(len(thermomut_df))\n",
        "print(len(fireprot_df))\n",
        "print(len(excluded_fire_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd5KH3aZ3J7N"
      },
      "outputs": [],
      "source": [
        "entry = excluded_fire_df.iloc[1]\n",
        "print(entry['uniprot_id'])\n",
        "print(entry['sequence'][entry['position'] - 1])\n",
        "print(entry['wild_type'], entry['position'], entry['mutation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHjZtsndpFnU"
      },
      "outputs": [],
      "source": [
        "entry = excluded_therm_df.iloc[4]\n",
        "print(entry['mutation_code'])\n",
        "print(entry['uniprot'])\n",
        "print(entry['sequence'][entry['position'] - 1])\n",
        "print(entry['sequence'])\n",
        "\n",
        "other_uni_entries = excluded_therm_df[entry['uniprot'] == excluded_therm_df['uniprot']]\n",
        "print(len(set(other_uni_entries['sequence'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYri2g-JCBBu"
      },
      "source": [
        "### Fill in Interpro Family Info for ThermoMutDB \n",
        "This code based on the InterPro program-friendly API:\n",
        "\n",
        "https://www.ebi.ac.uk/interpro/result/download/#/entry/InterPro/protein/UniProt/|json "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9L3ahjpKKwB"
      },
      "outputs": [],
      "source": [
        "# standard library modules\n",
        "import sys, errno, re, json, ssl\n",
        "from urllib import request\n",
        "from urllib.error import HTTPError\n",
        "from time import sleep\n",
        "\n",
        "def get_interpro_fams(start_url:str):\n",
        "  #disable SSL verification to avoid config issues\n",
        "  context = ssl._create_unverified_context()\n",
        "\n",
        "  next = start_url\n",
        "  last_page = False\n",
        "\n",
        "  \n",
        "  #json header\n",
        "  # sys.stdout.write(\"{ \\\"results\\\": [\\n\")\n",
        "  \n",
        "  attempts = 0\n",
        "  result = []\n",
        "  while next:\n",
        "    try:\n",
        "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
        "      res = request.urlopen(req, context=context)\n",
        "      # If the API times out due a long running query\n",
        "      if res.status == 408:\n",
        "        # wait just over a minute\n",
        "        sleep(61)\n",
        "        # then continue this loop with the same URL\n",
        "        continue\n",
        "      elif res.status == 204:\n",
        "        #no data so leave loop\n",
        "        break\n",
        "      payload = json.loads(res.read().decode())\n",
        "      next = payload[\"next\"]\n",
        "      attempts = 0\n",
        "      if not next:\n",
        "        last_page = True\n",
        "    except HTTPError as e:\n",
        "      if e.code == 408:\n",
        "        sleep(61)\n",
        "        continue\n",
        "      else:\n",
        "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
        "        if attempts < 3:\n",
        "          attempts += 1\n",
        "          sleep(61)\n",
        "          continue\n",
        "        else:\n",
        "          sys.stderr.write(\"LAST URL: \" + next)\n",
        "          raise e\n",
        "\n",
        "    for i, item in enumerate(payload[\"results\"]):\n",
        "      # sys.stdout.write(json.dumps(item))\n",
        "      result.append(item['metadata']['accession'])\n",
        "      # for indented output replace the above line with the following\n",
        "      # sys.stdout.write(json.dumps(item, indent=4))\n",
        "      # for 1 record per line uncomment the following line\n",
        "      # sys.stdout.write(\"\\n\")\n",
        "\n",
        "      \"\"\"\n",
        "      if last_page and i+1 == len(payload[\"results\"]):\n",
        "        sys.stdout.write(\"\")\n",
        "      else:\n",
        "        sys.stdout.write(\",\\n\")\n",
        "      \"\"\"\n",
        "      \n",
        "    # Don't overload the server, give it time before asking for more\n",
        "    if next:\n",
        "      sleep(1)\n",
        "\n",
        "  #json footer\n",
        "  # sys.stdout.write(\"\\n] }\\n\")\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1IHqekU2cFG"
      },
      "outputs": [],
      "source": [
        "def load_interpro_fams(uni_ids, save_file:str):\n",
        "  \"\"\"\n",
        "  Load Protein family information from Interpro database\n",
        "  :param uni_ids: set of strings, each string is a uniprot id\n",
        "  :return: dict, maps id to list of families\n",
        "  \"\"\"\n",
        "  if os.path.exists(save_file):\n",
        "    family_map = np.load(save_file, allow_pickle=True).item()\n",
        "    print(\"Successfully loaded file from:\", save_file)\n",
        "    return family_map\n",
        "  # query info from interpro and save to .npy file\n",
        "  family_map = {}\n",
        "  print(\"Loading data from Interpro...\")\n",
        "  for id in tqdm(uni_ids):\n",
        "    id_url = \"https://www.ebi.ac.uk:443/interpro/api/entry/InterPro/protein/UniProt/%s/?page_size=200\" % id\n",
        "    family_map[id] = get_interpro_fams(id_url)\n",
        "  np.save(save_file, family_map)\n",
        "  print(\"\\nSaved data to:\", save_file)\n",
        "  return family_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4kxvpadvNKM"
      },
      "outputs": [],
      "source": [
        "therm_ids = set(thermomut_df['uniprot'])\n",
        "save_file = os.path.join(DATA_DIR, \"thermomut_interpro_fams.npy\")\n",
        "therm_fams = load_interpro_fams(therm_ids, save_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5HScRaCBiNB"
      },
      "outputs": [],
      "source": [
        "thermomut_df['interpro_families'] = thermomut_df['uniprot'].map(therm_fams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mFI0KpARfz4"
      },
      "source": [
        "### Fix ddG values for FireProt\n",
        "FireProt considers destabilizing mutations to have ddG > 0, but other tools in this work consider destabilizing mutations to be ddG < 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaoumKsxRZea"
      },
      "outputs": [],
      "source": [
        "fireprot_df['ddG'] = -1.0 * fireprot_df['ddG'] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAJ5HxHS88ME"
      },
      "source": [
        "### Combine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot8slRQ_87g1"
      },
      "outputs": [],
      "source": [
        "useful_csv = \"data\\col_mapping.csv\"\n",
        "col_map_df = pd.read_csv(csv_export_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQJ9-U-PAGXc"
      },
      "outputs": [],
      "source": [
        "# cols to keep for each dataset\n",
        "useful_cols_df = col_map_df[col_map_df['useful'] == 1]\n",
        "fire_keep_cols = useful_cols_df['fireprot']\n",
        "therm_keep_cols = useful_cols_df['thermomut']\n",
        "\n",
        "fire_filt_df = fireprot_df[fire_keep_cols]\n",
        "therm_filt_df = thermomut_df[therm_keep_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myko47QKBKi7"
      },
      "outputs": [],
      "source": [
        "# change thermomut col names to match fireprot\n",
        "col_map = pd.Series(useful_cols_df.fireprot.values,index=useful_cols_df.thermomut).to_dict()\n",
        "therm_filt_df = therm_filt_df.rename(columns=col_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo7g74oNILAh"
      },
      "outputs": [],
      "source": [
        "fire_filt_df = fire_filt_df.assign(db_origin='fireprot')\n",
        "therm_filt_df = therm_filt_df.assign(db_origin='thermomut')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMwnpp3KXxOs"
      },
      "outputs": [],
      "source": [
        "# if the attributes below all match another entry then it's a duplicate\n",
        "match_cols = ['uniprot_id', 'position', 'wild_type', 'mutation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdqc6Lh2XlPr"
      },
      "outputs": [],
      "source": [
        "# drop duplicates from each df\n",
        "# for duplicates keep only the first entry in the database\n",
        "therm_filt_df = therm_filt_df.drop_duplicates(subset=match_cols, keep='first')\n",
        "fire_filt_df = fire_filt_df.drop_duplicates(subset=match_cols, keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYGb9khHSG3u"
      },
      "outputs": [],
      "source": [
        "# get unique entries in thermomut\n",
        "# code taken from: \n",
        "# https://stackoverflow.com/questions/44706485/how-to-remove-rows-in-a-pandas-dataframe-if-the-same-row-exists-in-another-dataf \n",
        "a = therm_filt_df\n",
        "b = fire_filt_df\n",
        "\n",
        "a_index = a.set_index(match_cols).index\n",
        "b_index = b.set_index(match_cols).index\n",
        "mask = ~a_index.isin(b_index)\n",
        "therm_filt_unique = a.loc[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9r8BCmC88I"
      },
      "outputs": [],
      "source": [
        "# combine fireprot entries with entries unique to thermomut\n",
        "comb_df = pd.concat([fire_filt_df, therm_filt_unique])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GsQPNyY_KME"
      },
      "outputs": [],
      "source": [
        "# validate that all sequences are the same for a given id\n",
        "uniprot_ids = set(comb_df['uniprot_id'])\n",
        "for ui in uniprot_ids:\n",
        "  unique_seqs = set(comb_df[comb_df['uniprot_id'] == ui]['sequence'])\n",
        "  if len(unique_seqs) != 1:\n",
        "    print(\"failed at: %s\" % ui)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUzxeMWfESJ8"
      },
      "outputs": [],
      "source": [
        "# save to csv file\n",
        "comb_save_pth = os.path.join(DATA_DIR, \"combined_cleaned.csv\")\n",
        "comb_df.to_csv(comb_save_pth, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKx60tst4lqs"
      },
      "source": [
        "### Add Small Prot Dataset\n",
        "Due to computational limitations will only use smaller proteins (<= 400 residues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLZHkOgT5Gil"
      },
      "outputs": [],
      "source": [
        "MAX_RESIDUES = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ra55iWL46MZ"
      },
      "outputs": [],
      "source": [
        "comb_save_pth = os.path.join(DATA_DIR, \"combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DhdPDAQ48K0"
      },
      "outputs": [],
      "source": [
        "# sequence lengths\n",
        "seq_lens = prot_df['sequence'].map(lambda x : len(x))\n",
        "# ignore large proteins (~500 are larger than 1000 residues)\n",
        "small_prots = prot_df.loc[seq_lens <= MAX_RESIDUES, :]\n",
        "small_prots['sequence_length'] = seq_lens.loc[seq_lens <= MAX_RESIDUES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA5FJypq537U"
      },
      "outputs": [],
      "source": [
        "print('min length:', min(small_prots['sequence_length']))\n",
        "print('max length:', max(small_prots['sequence_length']))\n",
        "print('n entries:', len(small_prots))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Av6mmsk55hb"
      },
      "outputs": [],
      "source": [
        "# save to csv file\n",
        "small_comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "small_prots.to_csv(small_comb_save_pth, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-x0Q42f-bVS"
      },
      "source": [
        "# ESMFold Setup\n",
        "Heavy-lifting done on CARC, here just prepping FASTA files for input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfdc2xzxX9gQ"
      },
      "source": [
        "### Create FASTA Files\n",
        "Generate FASTA files for usage with ESMFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQLAgyokbXam"
      },
      "outputs": [],
      "source": [
        "def gen_fasta_file(fpath: str, seqs: list, labels: list):\n",
        "  \"\"\"\n",
        "  Generate a FASTA file with the given params. Each sequence labeled\n",
        "  by uniprot id.\n",
        "  :param str fpath: path to save fasta file to\n",
        "  :param list seqs: sequences to write\n",
        "  :param labels: corresponding labels for each sequence\n",
        "  \"\"\"\n",
        "  with open(fpath, \"w\") as ofile:\n",
        "    for seq, id in zip(seqs, labels):\n",
        "      ofile.write(\">\" + id + \"\\n\" + seq + \"\\n\")\n",
        "  print(\"Sucessfully saved given seqs/ids to:\", fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M-yneTLYldO"
      },
      "outputs": [],
      "source": [
        "comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL4ehWWlYH-B"
      },
      "source": [
        "### Sample shorter Seqs for time benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EysfVqR6lSY2"
      },
      "outputs": [],
      "source": [
        "def get_sample_seqs(length: int, n_samples: int=1, eps: int=20):\n",
        "  \"\"\"\n",
        "  Get a protein sequence from prot_df of the desired length (+-eps)\n",
        "  \"\"\"\n",
        "  seq_lens = prot_df['sequence'].map(lambda x : len(x))\n",
        "  sample_df = prot_df[seq_lens >= (length - eps)]\n",
        "  sample_df = sample_df[seq_lens <= (length + eps)]\n",
        "  if len(sample_df) == 0:\n",
        "    # no entries of desired length found\n",
        "    return None\n",
        "  sampled_entry = sample_df.sample(n=n_samples, random_state=42).iloc[0]\n",
        "  return sampled_entry['sequence'], sampled_entry['uniprot_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1bsqJDxYktf",
        "outputId": "7ce4b3f6-8339-4844-d482-5b8f8a21116e"
      },
      "outputs": [],
      "source": [
        "desired_lens = np.arange(50, 450, 50)\n",
        "found_uni_ids = []\n",
        "found_seqs = []\n",
        "for length in desired_lens:\n",
        "  sample_seqs, ids = get_sample_seqs(length, n_samples=2)\n",
        "  if sample_seqs is not None:\n",
        "    found_seqs.append(sample_seqs)\n",
        "    found_uni_ids.append(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn8N6qaZa6Ie",
        "outputId": "411f2e31-94ff-4e47-b6db-2318a36a30cc"
      },
      "outputs": [],
      "source": [
        "print(len(found_seqs))\n",
        "print(found_uni_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJIR-O0qy-o9"
      },
      "source": [
        "For ESMFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpFPbBLscK43",
        "outputId": "fa1fa08b-7e62-453b-f285-0e3a9c0a50dd"
      },
      "outputs": [],
      "source": [
        "pth = os.path.join(DATA_DIR, \"benchmarking_seqs.fasta\")\n",
        "gen_fasta_file(pth, found_seqs, found_uni_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKiO1jnBBWvd"
      },
      "source": [
        "### Full Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taibo4IaC6qW"
      },
      "outputs": [],
      "source": [
        "# create dir for fasta files\n",
        "FASTA_DIR = os.path.join(DATA_DIR, \"esm_fastas\")\n",
        "WILDTYPE_DIR = os.path.join(FASTA_DIR, \"wildtypes\")\n",
        "MUT_DUR = os.path.join(FASTA_DIR, \"mutants\")\n",
        "dirs = [FASTA_DIR, WILDTYPE_DIR, MUT_DUR]\n",
        "for d in dirs:\n",
        "  os.makedirs(d, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6M2h01wD3zl"
      },
      "outputs": [],
      "source": [
        "def get_length_n_seqs(df: pd.DataFrame, seq_lens: pd.Series, lower_b: int, upper_b: int):\n",
        "  \"\"\"\n",
        "  Get all protein sequences from df of desired length (lower_b <= length < upper_b)\n",
        "  \"\"\"\n",
        "  ret_df = df.loc[seq_lens.between(lower_b, upper_b, inclusive='left')]\n",
        "  return ret_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09rGe4--DGyj"
      },
      "outputs": [],
      "source": [
        "def save_wildtypes(df: pd.DataFrame, fpath: str):\n",
        "  uni_ids = list(set(df['uniprot_id']))\n",
        "  wild_seqs = []\n",
        "  # this is needed bc a small minority of uni-ids share the same sequence\n",
        "  for ui in uni_ids:\n",
        "    seq = df[df['uniprot_id'] == ui]['sequence'].iloc[0]\n",
        "    wild_seqs.append(seq)\n",
        "  assert len(wild_seqs) == len(uni_ids)\n",
        "  print(\"Num. wild-seqs:\", len(uni_ids))\n",
        "  gen_fasta_file(fpath, wild_seqs, uni_ids)\n",
        "  return len(uni_ids)\n",
        "\n",
        "\n",
        "def get_mut_seq(seq: str, position: int, mut_res: str, wild_res: str):\n",
        "  if position >= len(seq) or seq[position] != wild_res:\n",
        "    raise ValueError(\"Invalid, given position was %d\", position)\n",
        "  return seq[:position] + mut_res + seq[position+1:]\n",
        "\n",
        "def save_mutants(df: pd.DataFrame, fpath: str):\n",
        "  mutant_seqs = df.apply(lambda x: get_mut_seq(x['sequence'], x['position'] - 1, x['mutation'], x['wild_type']), axis=1)\n",
        "  mutant_seqs = list(mutant_seqs)\n",
        "  labels = df.apply(lambda x: \"%s_%s%d%s\" % (x['uniprot_id'], x['wild_type'], x['position'], x['mutation']), axis=1)\n",
        "  print(\"Num. mutant seqs:\", len(mutant_seqs))\n",
        "  gen_fasta_file(fpath, mutant_seqs, labels)\n",
        "  return len(mutant_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiEkyWwbBao3"
      },
      "outputs": [],
      "source": [
        "seq_lens = prot_df['sequence_length']\n",
        "max_len = max(seq_lens)\n",
        "print(max_len)\n",
        "\n",
        "# chunk data into different lengths, \n",
        "# so prots with 0-50 residues go into one file, 50-100 another, etc\n",
        "skip = 50\n",
        "bounds = np.arange(0, max_len+skip, skip)\n",
        "mut_tot = 0\n",
        "wild_tot = 0\n",
        "for i in range(1, len(bounds)):\n",
        "  lb = bounds[i-1]\n",
        "  ub = bounds[i]\n",
        "  print(\"Range: [%d, %d)\" % (lb, ub))\n",
        "  s_df = get_length_n_seqs(prot_df, seq_lens, lb, ub)\n",
        "  fname = \"length_%d_%d.fasta\" % (lb, ub - 1)\n",
        "  wild_fpath = os.path.join(WILDTYPE_DIR, fname)\n",
        "  mut_fpath = os.path.join(MUT_DUR, fname)\n",
        "  wild_tot += save_wildtypes(s_df, wild_fpath)\n",
        "  mut_tot += save_mutants(s_df, mut_fpath)\n",
        "  print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNOOdjix4NNr"
      },
      "outputs": [],
      "source": [
        "expected_wild_tot = len(set(prot_df['uniprot_id']))\n",
        "expected_mut_tot = len(prot_df)\n",
        "print(expected_wild_tot)\n",
        "print(expected_mut_tot)\n",
        "print(wild_tot)\n",
        "print(mut_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx5EOJyj7jIe"
      },
      "outputs": [],
      "source": [
        "uids = list(set(prot_df['uniprot_id']))\n",
        "for i in tqdm(range(len(uids))):\n",
        "  x = uids[i]\n",
        "  for j in range(i+1, len(uids)):\n",
        "    y = uids[j]\n",
        "    x_seq = set(prot_df[prot_df['uniprot_id'] == x]['sequence'])\n",
        "    y_seq = set(prot_df[prot_df['uniprot_id'] == y]['sequence'])\n",
        "    both = x_seq & y_seq\n",
        "    if len(both) > 0:\n",
        "      print(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ3RocWKLUXL"
      },
      "outputs": [],
      "source": [
        "# see where mut-position is > sequence\n",
        "p_df = prot_df[prot_df['sequence_length'] > prot_df['position']]\n",
        "w_types = p_df['wild_type']\n",
        "positions = p_df['position']\n",
        "\n",
        "seqs = p_df['sequence']\n",
        "s_types = [x[p-1] for x, p in zip(seqs, positions)]\n",
        "\n",
        "bad_locs = p_df[w_types != s_types]\n",
        "good_locs = p_df[w_types == s_types]\n",
        "\n",
        "print(len(bad_locs))\n",
        "print(np.unique(good_locs['db_origin'], return_counts=True))\n",
        "print(np.unique(bad_locs['db_origin'], return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6lpRf7p-kvZ"
      },
      "source": [
        "# AlphaFold\n",
        "Download wild-type predictions from AlphaFold database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgIRWhzayRH2"
      },
      "outputs": [],
      "source": [
        "small_comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "small_prot_df = pd.read_csv(small_comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHoMaDq3nRCY"
      },
      "outputs": [],
      "source": [
        "# adapted from:\n",
        "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
        "\n",
        "def download_alphafold_pdb(url: str, uniprot_id: str, save_pth: str):\n",
        "  try:\n",
        "    with requests.get(url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(save_pth, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192): \n",
        "          f.write(chunk)\n",
        "    return True\n",
        "  except requests.exceptions.HTTPError:\n",
        "    # protein not found in database\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lidbPCd6dsjX",
        "outputId": "eb593f63-affb-4963-c599-0dae6eb7ddfc"
      },
      "outputs": [],
      "source": [
        "# setup directory to save predictions to\n",
        "ALPHAFOLD_DIR = os.path.join(DATA_DIR, \"alphafold_preds\", \"wildtypes\")\n",
        "os.makedirs(ALPHAFOLD_DIR, exist_ok=True)\n",
        "\n",
        "# uniprot ids not found in database\n",
        "not_found = [] \n",
        "\n",
        "# save all wild-type predictions from alphafold db\n",
        "uniprot_ids = set(prot_df['uniprot_id'])\n",
        "for uid in tqdm(uniprot_ids):\n",
        "  url_target = \"https://alphafold.ebi.ac.uk/files/AF-%s-F1-model_v4.pdb\" % uid\n",
        "  fname = \"%s.pdb\" % uid\n",
        "  save_pth = os.path.join(ALPHAFOLD_DIR, fname)\n",
        "  found = download_alphafold_pdb(url_target, uid, save_pth)\n",
        "  if not(found):\n",
        "    not_found.append(uid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssbFFXaa8izt",
        "outputId": "86bc98cc-9623-40a5-bca0-64db7c735f22"
      },
      "outputs": [],
      "source": [
        "print(sorted(not_found))\n",
        "print(len(not_found))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rb5gv5d-CEm",
        "outputId": "15ac6e42-01ed-4505-ea39-1232c93341db"
      },
      "outputs": [],
      "source": [
        "filt_prot_df = prot_df.loc[prot_df['uniprot_id'].isin(not_found), :]\n",
        "print(len(filt_prot_df))\n",
        "print(sorted(set(filt_prot_df['uniprot_id'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHNNoYP7ePkr"
      },
      "source": [
        "# Generate fastas for HH-Suite\n",
        "Heavy-lifting done on CARC, here just prepping FASTA files for input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IPXRTNne5uU"
      },
      "outputs": [],
      "source": [
        "def gen_hh_fasta_file(fpath: str, seq: str, label: str):\n",
        "  \"\"\"\n",
        "  Generate a FASTA file with the given params. Each sequence labeled\n",
        "  by uniprot id.\n",
        "  :param str fpath: path to save fasta file to\n",
        "  :param str seqs: sequence to write\n",
        "  :param str label: label for sequence\n",
        "  \"\"\"\n",
        "  with open(fpath, \"w\") as ofile:\n",
        "    ofile.write(\">\" + label + \"\\n\" + seq + \"\\n\")\n",
        "  return True # success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KO_ZGdafWaX"
      },
      "outputs": [],
      "source": [
        "def get_length_n_seqs(df: pd.DataFrame, seq_lens: pd.Series, lower_b: int, upper_b: int):\n",
        "  \"\"\"\n",
        "  Get all protein sequences from df of desired length (lower_b <= length < upper_b)\n",
        "  \"\"\"\n",
        "  ret_df = df.loc[seq_lens.between(lower_b, upper_b, inclusive='left')]\n",
        "  return ret_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_bp_6Qjf6KY"
      },
      "outputs": [],
      "source": [
        "def save_wildtypes(df: pd.DataFrame, save_dir: str):\n",
        "  uni_ids = list(set(df['uniprot_id']))\n",
        "  wild_seqs = []\n",
        "  for ui in uni_ids:\n",
        "    seq = df[df['uniprot_id'] == ui]['sequence'].iloc[0]\n",
        "    wild_seqs.append(seq)\n",
        "  assert len(wild_seqs) == len(uni_ids)\n",
        "  print(\"Num. wild-seqs:\", len(uni_ids))\n",
        "  for seq, uid in zip(wild_seqs, uni_ids):\n",
        "    fpath = os.path.join(save_dir, uid + \".fasta\")\n",
        "    gen_hh_fasta_file(fpath, seq, uid)\n",
        "  return len(uni_ids)\n",
        "\n",
        "\n",
        "def get_mut_seq(seq: str, position: int, mut_res: str, wild_res: str):\n",
        "  if position >= len(seq) or seq[position] != wild_res:\n",
        "    raise ValueError(\"Invalid, given position was %d\", position)\n",
        "  return seq[:position] + mut_res + seq[position+1:]\n",
        "\n",
        "def save_mutants(df: pd.DataFrame, save_dir: str):\n",
        "  mutant_seqs = df.apply(lambda x: get_mut_seq(x['sequence'], x['position'] - 1, x['mutation'], x['wild_type']), axis=1)\n",
        "  mutant_seqs = list(mutant_seqs)\n",
        "  labels = df.apply(lambda x: \"%s_%s%d%s\" % (x['uniprot_id'], x['wild_type'], x['position'], x['mutation']), axis=1)\n",
        "  print(\"Num. mutant seqs:\", len(mutant_seqs))\n",
        "  for seq, label in zip(mutant_seqs, labels):\n",
        "    fpath = os.path.join(save_dir, label + \".fasta\")\n",
        "    gen_hh_fasta_file(fpath, seq, label)\n",
        "  return len(mutant_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-9ZeYaafk9V"
      },
      "outputs": [],
      "source": [
        "# create dir for fasta files\n",
        "FASTA_DIR = os.path.join(DATA_DIR, \"hhsuite_fastas\")\n",
        "WILDTYPE_DIR = os.path.join(FASTA_DIR, \"wildtypes\")\n",
        "MUT_DUR = os.path.join(FASTA_DIR, \"mutants\")\n",
        "dirs = [FASTA_DIR, WILDTYPE_DIR, MUT_DUR]\n",
        "for d in dirs:\n",
        "  os.makedirs(d, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4342zLlh-PM"
      },
      "outputs": [],
      "source": [
        "comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AafxBPKhpC8"
      },
      "outputs": [],
      "source": [
        "seq_lens = prot_df['sequence_length']\n",
        "max_len = max(seq_lens)\n",
        "print(max_len)\n",
        "\n",
        "# chunk data into different lengths, \n",
        "# so prots with 0-50 residues go into one file, 50-100 another, etc\n",
        "skip = 50\n",
        "bounds = np.arange(0, max_len+skip, skip)\n",
        "mut_tot = 0\n",
        "wild_tot = 0\n",
        "for i in range(1, len(bounds)):\n",
        "  lb = bounds[i-1]\n",
        "  ub = bounds[i]\n",
        "  print(\"Range: [%d, %d)\" % (lb, ub))\n",
        "  s_df = get_length_n_seqs(prot_df, seq_lens, lb, ub)\n",
        "  subdir = \"length_%d_%d\" % (lb, ub - 1)\n",
        "  wild_tot += save_wildtypes(s_df, WILDTYPE_DIR)\n",
        "  mut_tot += save_mutants(s_df, MUT_DUR)\n",
        "  print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L15wZWxA56M"
      },
      "source": [
        "# PDB Prediction Data\n",
        "Gather plDDt scores and other relevant info for predictions from ESMFold and AlphaFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJEsatGSkmVQ"
      },
      "source": [
        "### Get average pLDDT values from predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3ziczlMCe3K"
      },
      "outputs": [],
      "source": [
        "ALPHAFOLD_DIR = os.path.join(DATA_DIR, \"alphafold\")\n",
        "ALPHA_WILDTYPE_DIR = os.path.join(ALPHAFOLD_DIR, \"alphafold_preds\", \"wildtypes\")\n",
        "\n",
        "ESMFOLD_DIR = os.path.join(DATA_DIR, \"esmfold\")\n",
        "ESM_WILDTYPE_DIR = os.path.join(ESMFOLD_DIR, \"esm_preds\", \"wildtypes\")\n",
        "ESM_MUTANT_DIR = os.path.join(ESMFOLD_DIR, \"esm_preds\", \"mutants\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buZ_hDP1bKjW"
      },
      "outputs": [],
      "source": [
        "def get_struct(pdb_path: str) -> Bio.PDB.Structure:\n",
        "  \"\"\"\n",
        "  Load a pdb file from the given path into a Bio.PDB.Structure object\n",
        "  :return: Bio.PDB.Structure object\n",
        "  \"\"\"\n",
        "  builder = Bio.PDB.Polypeptide.PPBuilder()\n",
        "  parser = Bio.PDB.PDBParser(QUIET=True)\n",
        "  struct = parser.get_structure('Structure', pdb_path)\n",
        "  return struct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blx4uo4Zb4Uy"
      },
      "outputs": [],
      "source": [
        "# credit to ChatGPT for writing this function\n",
        "def get_plddts(struct: Bio.PDB.Structure):\n",
        "    \"\"\"\n",
        "    Extracts the plddt values from an AlphaFold PDB structure.\n",
        "    :param struct: A Bio.PDB.Structure object.\n",
        "    :return: A list of floats representing the plddt values in `struct`.\n",
        "    \"\"\"\n",
        "    # get the first model\n",
        "    model = struct[0]\n",
        "\n",
        "\n",
        "    # create an empty list to store the pLDDT values\n",
        "    plddt_values = []\n",
        "\n",
        "    # iterate over each residue in the model\n",
        "    for residue in model.get_residues():\n",
        "        # get the B-factor value for the residue\n",
        "        bfactor = residue[\"CA\"].get_bfactor()\n",
        "\n",
        "        # append the B-factor value to the pLDDT values list\n",
        "        plddt_values.append(bfactor)\n",
        "\n",
        "    return plddt_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLkgJ03kWTYJ"
      },
      "outputs": [],
      "source": [
        "def get_avg_plddts(inp_dir: str) -> list:\n",
        "  \"\"\"\n",
        "  Get a list of average plddt values for each pdb structure in a given directory\n",
        "  :param str inp_dir: input directory containing pdbs\n",
        "  :return: list[float], average plddt values for each pdb in the directory\n",
        "  \"\"\"\n",
        "  avg_plddts = []\n",
        "  pdb_files = []\n",
        "  for pdb_file in tqdm(os.listdir(inp_dir)):\n",
        "    if not(pdb_file.endswith(\".pdb\")):\n",
        "      continue # don't want to try to get plddt of log files and such\n",
        "    pdb_path = os.path.join(inp_dir, pdb_file)\n",
        "    struct = get_struct(pdb_path)\n",
        "    avg_plddt = np.mean(get_plddts(struct))\n",
        "    avg_plddts.append(avg_plddt)\n",
        "    pdb_files.append(pdb_file)\n",
        "  return avg_plddts, pdb_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wHX7jBkcGFe",
        "outputId": "7bfb7771-60da-42a6-b06b-f5177bde7053"
      },
      "outputs": [],
      "source": [
        "alpha_wild_plddts, alpha_wild_pdb_files = get_avg_plddts(ALPHA_WILDTYPE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdYBWW5RXiXq"
      },
      "outputs": [],
      "source": [
        "# had stored pdbs in subdirs based on seq length, hence the loops\n",
        "def iterate_subdirs(dir: str):\n",
        "  plddts = []\n",
        "  pdb_files = []\n",
        "  for subdir in os.listdir(dir):\n",
        "    sub_plddts, sub_pdb_files = get_avg_plddts(os.path.join(dir, subdir))\n",
        "    plddts += sub_plddts\n",
        "    pdb_files += sub_pdb_files\n",
        "  return plddts, pdb_files\n",
        "\n",
        "esm_wild_plddts, esm_wild_pdb_files = iterate_subdirs(ESM_WILDTYPE_DIR)\n",
        "esm_mutant_plddts, esm_mutant_pdb_files = iterate_subdirs(ESM_MUTANT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkHYrcv7hShg"
      },
      "source": [
        "### Saving into csvs for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUfjKvvYhWvg"
      },
      "outputs": [],
      "source": [
        "def save_plddt_wild_data(plddt_vals: list, pdb_files: list, save_path: str):\n",
        "  uni_ids = [s[:s.index(\".pdb\")] for s in pdb_files]\n",
        "  df = pd.DataFrame(list(zip(plddt_vals, uni_ids)), columns=[\"avg_plddt\", \"uniprot_id\"])\n",
        "  df.to_csv(save_path, index=False)\n",
        "\n",
        "def save_plddt_mut_data(plddt_vals: list, pdb_files: list, save_path: str):\n",
        "  uni_ids = [s[:s.index(\".pdb\")].split(\"_\")[0] for s in pdb_files]\n",
        "  mut_codes = [s[:s.index(\".pdb\")].split(\"_\")[1] for s in pdb_files]\n",
        "  df = pd.DataFrame(list(zip(plddt_vals, uni_ids, mut_codes)), columns=[\"avg_plddt\", \"uniprot_id\", \"mut_code\"])\n",
        "  df.to_csv(save_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8am_7MsrVMYo"
      },
      "outputs": [],
      "source": [
        "save_plddt_wild_data(alpha_wild_plddts, alpha_wild_pdb_files, os.path.join(ALPHAFOLD_DIR, \"plddts.csv\"))\n",
        "save_plddt_wild_data(esm_wild_plddts, esm_wild_pdb_files, os.path.join(ESM_WILDTYPE_DIR, \"plddts.csv\"))\n",
        "save_plddt_mut_data(esm_mutant_plddts, esm_mutant_pdb_files, os.path.join(ESM_MUTANT_DIR, \"plddts.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6CPbKqbl--T"
      },
      "source": [
        "# DDGun Setup for CARC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oumeEicjKT0E"
      },
      "source": [
        "### Create file formats expected by ddgun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gilppg0_mHy"
      },
      "outputs": [],
      "source": [
        "DDGUN_DIR = os.path.join(DATA_DIR, \"ddgun\")\n",
        "MUTFILE_DIR = os.path.join(DDGUN_DIR, \"mut_files\")\n",
        "os.makedirs(MUTFILE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FckoWAK6BV39"
      },
      "outputs": [],
      "source": [
        "comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgSr8cFrBY5F"
      },
      "outputs": [],
      "source": [
        "def write_mut_file(file_path: str, muts: list):\n",
        "  with open(file_path, 'w') as outfile:\n",
        "    outfile.write('\\n'.join(muts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coCRC6tCUFF",
        "outputId": "7e2489bc-de4c-4c14-c38e-6e84f3c8b593"
      },
      "outputs": [],
      "source": [
        "uni_ids = set(prot_df['uniprot_id'])\n",
        "for uni_id in tqdm(uni_ids):\n",
        "  sub_df = prot_df.loc[prot_df['uniprot_id'] == uni_id]\n",
        "  mut_codes = list(sub_df.apply(lambda x: \"%s%d%s\" % (x['wild_type'], x['position'], x['mutation']), axis=1))\n",
        "  fname = \"%s.muts\" % uni_id\n",
        "  save_path = os.path.join(MUTFILE_DIR, fname)\n",
        "  write_mut_file(save_path, mut_codes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpjl9AuG_hck"
      },
      "source": [
        "# DDGun / ACDC-NN Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZDWlQWjLQit"
      },
      "source": [
        "### Glob together ddgun predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzVukuvFLWTb"
      },
      "outputs": [],
      "source": [
        "DDGUN_DIR = os.path.join(DATA_DIR, \"ddgun\")\n",
        "DDGUN_OUT_DIR = os.path.join(DDGUN_DIR, \"out_files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBO8cK3yL9bd"
      },
      "outputs": [],
      "source": [
        "def generate_output_predictions(inp_dir: str):\n",
        "  \"\"\"\n",
        "  Concat together all of the .out files in a directory into a single DataFrame\n",
        "  :param str inp_dir: path to input directory\n",
        "  :return: pd.DataFrame containing all of the predictions w/ uniprot_id, chain, and mutation code\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(columns=['uniprot_id', 'CHAIN', 'VARIANT', 'T_DDG[3D]'])\n",
        "  dfs = []\n",
        "  for file_name in tqdm(os.listdir(inp_dir)):\n",
        "      file_path = os.path.join(inp_dir, file_name)\n",
        "      temp_df = pd.read_csv(file_path, sep='\\t', skiprows=1, header=None, names=['PDBFILE', 'CHAIN', 'VARIANT', 'S_DDG[3D]', 'T_DDG[3D]', 'STABILITY[3D]'])\n",
        "      uniprot_id = os.path.splitext(file_name)[0]\n",
        "      temp_df['uniprot_id'] = uniprot_id\n",
        "      dfs.append(temp_df[['uniprot_id', 'CHAIN', 'VARIANT', 'T_DDG[3D]']])\n",
        "  df = pd.concat(dfs)\n",
        "  df = df.rename(columns={'CHAIN': 'chain', 'VARIANT': 'mut_code', 'T_DDG[3D]':'predicted_ddg'})\n",
        "  df = df.reset_index(drop=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGrRp2AUNdBW"
      },
      "outputs": [],
      "source": [
        "def generate_output_predictions_seq(inp_dir: str):\n",
        "  \"\"\"\n",
        "  Concat together all of the .out files in a directory into a single DataFrame\n",
        "  :param str inp_dir: path to input directory\n",
        "  :return: pd.DataFrame containing all of the predictions w/ uniprot_id, chain, and mutation code\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(columns=['uniprot_id', 'VARIANT', 'T_DDG[SEQ]'])\n",
        "  dfs = []\n",
        "  for file_name in tqdm(os.listdir(inp_dir)):\n",
        "      file_path = os.path.join(inp_dir, file_name)\n",
        "      temp_df = pd.read_csv(file_path, sep='\\t', skiprows=1, header=None, names=['SEQFILE', 'VARIANT', 'S_DDG[SEQ]', 'T_DDG[SEQ]', 'STABILITY[SEQ]'])\n",
        "      uniprot_id = os.path.splitext(file_name)[0]\n",
        "      temp_df['uniprot_id'] = uniprot_id\n",
        "      dfs.append(temp_df[['uniprot_id', 'VARIANT', 'T_DDG[SEQ]']])\n",
        "  df = pd.concat(dfs)\n",
        "  df = df.rename(columns={'VARIANT': 'mut_code', 'T_DDG[SEQ]':'predicted_ddg'})\n",
        "  df = df.reset_index(drop=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ot7Bc_LLUG",
        "outputId": "700ad1d8-e2e1-461d-a30e-86ad11f691e9"
      },
      "outputs": [],
      "source": [
        "alphafold_dir = os.path.join(DDGUN_OUT_DIR, \"alphafold\")\n",
        "esmfold_dir = os.path.join(DDGUN_OUT_DIR, \"esmfold\")\n",
        "ddgun_alpha = generate_output_predictions(alphafold_dir) # alphafold\n",
        "ddgun_esm = generate_output_predictions(esmfold_dir) # esmfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrxjRRyvNJAt",
        "outputId": "b59f7875-0785-48ca-e4c6-74fec39e7c67"
      },
      "outputs": [],
      "source": [
        "seq_dir = os.path.join(DDGUN_OUT_DIR, \"sequence\")\n",
        "ddgun_seq = generate_output_predictions_seq(seq_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB9mulsUObji"
      },
      "outputs": [],
      "source": [
        "comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Zh-W0OhJaW"
      },
      "outputs": [],
      "source": [
        "# add in experimental ddg values\n",
        "prot_df['mut_code'] = prot_df.apply(lambda x: \"%s%d%s\" % (x['wild_type'], x['position'], x['mutation']), axis=1)\n",
        "\n",
        "ddgun_merged_alpha = pd.merge(ddgun_alpha, prot_df, how='left', on=['uniprot_id', 'mut_code'])\n",
        "# alphafold and esmfold only use chain 'A', hence chain_x being used here\n",
        "ddgun_alpha = ddgun_merged_alpha[['uniprot_id', 'chain_x', 'mut_code', 'ddG', 'predicted_ddg']]\n",
        "\n",
        "ddgun_merged_esm = pd.merge(ddgun_esm, prot_df, how='left', on=['uniprot_id', 'mut_code'])\n",
        "ddgun_esm = ddgun_merged_esm[['uniprot_id', 'chain_x', 'mut_code', 'ddG', 'predicted_ddg']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXfsvSoWZU8D"
      },
      "outputs": [],
      "source": [
        "ddgun_merged_seq = pd.merge(ddgun_seq, prot_df, how='left', on=['uniprot_id', 'mut_code'])\n",
        "ddgun_seq = ddgun_merged_seq[['uniprot_id', 'mut_code', 'ddG', 'predicted_ddg']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMnMbMIc2Oms"
      },
      "outputs": [],
      "source": [
        "# make naming consistent with acdcnn\n",
        "ddgun_alpha = ddgun_alpha.rename(columns={\"ddG\":\"experimental_ddg\"})\n",
        "ddgun_esm = ddgun_esm.rename(columns={\"ddG\":\"experimental_ddg\"})\n",
        "ddgun_seq = ddgun_seq.rename(columns={\"ddG\":\"experimental_ddg\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gtAb947ozak"
      },
      "source": [
        "### Make data consistent\n",
        "This is really important!! Want to compare datapoints that use the same set of mutations and wild-type proteins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIhrQn6DrbJN"
      },
      "outputs": [],
      "source": [
        "# read in acdcnn data\n",
        "ACDCNN_DIR = os.path.join(DATA_DIR, \"acdcnn\")\n",
        "ALPHAFOLD_DIR = os.path.join(ACDCNN_DIR, \"alphafold\", \"wildtypes\")\n",
        "ESMFOLD_DIR = os.path.join(ACDCNN_DIR, \"esmfold\", \"wildtypes\")\n",
        "ESMFOLD_MUT_DIR = os.path.join(ACDCNN_DIR, \"esmfold\", \"mutants\") # using mutant structure + wildtype\n",
        "alphafold_pred_csv = os.path.join(ALPHAFOLD_DIR, \"ddg_predictions.csv\")\n",
        "esmfold_wild_pred_csv = os.path.join(ESMFOLD_DIR, \"ddg_predictions.csv\")\n",
        "esmfold_mut_pred_csv = os.path.join(ESMFOLD_MUT_DIR, \"ddg_predictions.csv\")\n",
        "\n",
        "# alphafold wildtypes\n",
        "acdcnn_alpha = pd.read_csv(alphafold_pred_csv)\n",
        "acdcnn_alpha = acdcnn_alpha.rename(columns={'id':'uniprot_id'})\n",
        "\n",
        "# esmfold wildtypes\n",
        "acdcnn_esm_wild = pd.read_csv(esmfold_wild_pred_csv)\n",
        "acdcnn_esm_wild = acdcnn_esm_wild.rename(columns={'id':'uniprot_id'})\n",
        "\n",
        "# esmfold wildtype + mut structure\n",
        "acdcnn_esm_mut = pd.read_csv(esmfold_mut_pred_csv)\n",
        "acdcnn_esm_mut = acdcnn_esm_mut.rename(columns={'id':'uniprot_id'})\n",
        "\n",
        "# sequence-based\n",
        "acdcnn_seq_wild = pd.read_csv(os.path.join(ACDCNN_DIR, \"sequence\", \"ddg_predictions.csv\"))\n",
        "acdcnn_seq_wild =  acdcnn_seq_wild.rename(columns={'id':'uniprot_id'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekJf2hIarRLA"
      },
      "outputs": [],
      "source": [
        "# read-in protein data\n",
        "comb_save_pth = os.path.join(DATA_DIR, \"small_combined_cleaned.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssraUIftk8N3"
      },
      "outputs": [],
      "source": [
        "# read in plddt data\n",
        "ALPHAFOLD_DIR = os.path.join(DATA_DIR, \"alphafold\")\n",
        "ALPHA_WILDTYPE_DIR = os.path.join(ALPHAFOLD_DIR, \"alphafold_preds\", \"wildtypes\")\n",
        "ESMFOLD_DIR = os.path.join(DATA_DIR, \"esmfold\")\n",
        "ESM_WILDTYPE_DIR = os.path.join(ESMFOLD_DIR, \"esm_preds\", \"wildtypes\")\n",
        "ESM_MUTANT_DIR = os.path.join(ESMFOLD_DIR, \"esm_preds\", \"mutants\")\n",
        "\n",
        "alpha_plddts = pd.read_csv(os.path.join(ALPHAFOLD_DIR, \"plddts.csv\"))\n",
        "esm_wild_plddts = pd.read_csv(os.path.join(ESM_WILDTYPE_DIR, \"plddts.csv\"))\n",
        "esm_mut_plddts = pd.read_csv(os.path.join(ESM_MUTANT_DIR, \"plddts.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhhcTEJvgzho"
      },
      "outputs": [],
      "source": [
        "# ensure both dfs contain the predictions from the same set of ids/mutations\n",
        "def make_dfs_consistent(df1: pd.DataFrame, df2: pd.DataFrame, match_cols=['uniprot_id', 'mut_code']):\n",
        "  df1_index = df1.set_index(match_cols).index\n",
        "  df2_index = df2.set_index(match_cols).index\n",
        "  df1_mask = df1_index.isin(df2_index)\n",
        "  df2_mask = df2_index.isin(df1_index)\n",
        "  consistent_df1 = df1.loc[df1_mask]\n",
        "  consistent_df2 = df2.loc[df2_mask]\n",
        "  return consistent_df1, consistent_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avG9uwPtrFLP"
      },
      "outputs": [],
      "source": [
        "# go through several rounds to ensure all dfs are consistent\n",
        "# (there's probably a better way to do this)\n",
        "\n",
        "# r1\n",
        "ddgun_alpha_c, ddgun_esm_c = make_dfs_consistent(ddgun_alpha, ddgun_esm)\n",
        "acdcnn_alpha_c, acdcnn_esm_c = make_dfs_consistent(acdcnn_alpha, acdcnn_esm_wild)\n",
        "acdcnn_esm_mut_c, acdcnn_esm_c = make_dfs_consistent(acdcnn_esm_mut, acdcnn_esm_c)\n",
        "\n",
        "# r2\n",
        "acdcnn_alpha_c, acdcnn_esm_mut_c = make_dfs_consistent(acdcnn_alpha_c, acdcnn_esm_mut_c)\n",
        "ddgun_alpha_c, acdcnn_esm_mut_c = make_dfs_consistent(ddgun_alpha_c, acdcnn_esm_mut_c)\n",
        "ddgun_esm_c, acdcnn_esm_mut_c = make_dfs_consistent(ddgun_esm_c, acdcnn_esm_mut_c)\n",
        "\n",
        "# make prot_df consistent\n",
        "prot_df_c, acdcnn_esm_mut_c = make_dfs_consistent(prot_df, acdcnn_esm_mut_c)\n",
        "\n",
        "# make sequence-based consistent (seq-based contained same muts as original prot_df)\n",
        "acdcnn_seq_wild_c, acdcnn_esm_mut_c = make_dfs_consistent(acdcnn_seq_wild, acdcnn_esm_mut_c)\n",
        "ddgun_seq_c, acdcnn_esm_mut_c = make_dfs_consistent(ddgun_seq, acdcnn_esm_mut_c)\n",
        "\n",
        "# make plddt data consistent\n",
        "alpha_plddts_c, acdcnn_esm_mut_c = make_dfs_consistent(alpha_plddts, acdcnn_esm_mut_c, match_cols=\"uniprot_id\")\n",
        "esm_wild_plddts_c, acdcnn_esm_mut_c = make_dfs_consistent(esm_wild_plddts, acdcnn_esm_mut_c, match_cols=\"uniprot_id\")\n",
        "esm_mut_plddts, acdcnn_esm_mut_c = make_dfs_consistent(esm_mut_plddts, acdcnn_esm_mut_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcuaxL84uRYs"
      },
      "outputs": [],
      "source": [
        "# make sure all dfs have same num. uniprot ids and mutations\n",
        "dfs = [ddgun_alpha_c, ddgun_esm_c, ddgun_seq_c, acdcnn_alpha_c, acdcnn_esm_c, acdcnn_esm_mut_c, acdcnn_seq_wild_c, \n",
        "       prot_df_c]\n",
        "for df in dfs:\n",
        "  print(len(df))\n",
        "  print(len(set(df['uniprot_id'])))\n",
        "  print(\"-\"*60)\n",
        "\n",
        "plddt_dfs = [alpha_plddts_c, esm_wild_plddts_c, esm_mut_plddts]\n",
        "for df in plddt_dfs:\n",
        "  print(len(df)) # should be 184 for wild-type plddt data\n",
        "  print(len(set(df['uniprot_id'])))\n",
        "  print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFHHXDfpxbxQ"
      },
      "outputs": [],
      "source": [
        "# save final dfs\n",
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "dfs = [ddgun_alpha_c, ddgun_esm_c, acdcnn_alpha_c, acdcnn_esm_c, acdcnn_esm_mut_c, ddgun_seq_c, acdcnn_seq_wild_c, \n",
        "       alpha_plddts_c, esm_wild_plddts_c, esm_mut_plddts, prot_df_c]\n",
        "save_paths = [os.path.join(SAVE_DIR, \"ddgun_alphafold.csv\"), os.path.join(SAVE_DIR, \"ddgun_esmfold.csv\"),  \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_alphafold.csv\"), os.path.join(SAVE_DIR, \"acdcnn_esmfold.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_esmfold_muts.csv\"), os.path.join(SAVE_DIR, \"ddgun_seq.csv\"),\n",
        "              os.path.join(SAVE_DIR, \"acdcnn_seq.csv\"), os.path.join(SAVE_DIR, \"alphafold_wild_plddts.csv\"),\n",
        "              os.path.join(SAVE_DIR, \"esmfold_wild_plddts.csv\"), os.path.join(SAVE_DIR, \"esmfold_mutant_plddts.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"final_prot_df.csv\")]\n",
        "for df, save_path in zip(dfs, save_paths):\n",
        "  df.to_csv(save_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5NyLEoNKVbV"
      },
      "source": [
        "### Prediction evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM398iad354k"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vmhyqyY1jul"
      },
      "outputs": [],
      "source": [
        "# load predictions in \n",
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "\n",
        "save_paths = [os.path.join(SAVE_DIR, \"ddgun_alphafold.csv\"), os.path.join(SAVE_DIR, \"ddgun_esmfold.csv\"),  \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_alphafold.csv\"), os.path.join(SAVE_DIR, \"acdcnn_esmfold.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_esmfold_muts.csv\"), os.path.join(SAVE_DIR, \"ddgun_seq.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_seq.csv\")]\n",
        "\n",
        "ddgun_alpha = pd.read_csv(save_paths[0])\n",
        "ddgun_esm = pd.read_csv(save_paths[1])\n",
        "acdcnn_alpha = pd.read_csv(save_paths[2])\n",
        "acdcnn_esm = pd.read_csv(save_paths[3])\n",
        "acdcnn_esm_mut = pd.read_csv(save_paths[4])\n",
        "ddgun_seq = pd.read_csv(save_paths[5])\n",
        "acdcnn_seq = pd.read_csv(save_paths[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICkbAg8ksP5A"
      },
      "outputs": [],
      "source": [
        "FIG_SAVE_DIR = os.path.join(DATA_DIR, \"figures\", \"parity_plots\")\n",
        "os.makedirs(FIG_SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll62hB4-1Rpl"
      },
      "outputs": [],
      "source": [
        "def create_parity_plot(df: pd.DataFrame, save_pth: str=None, title: str=None):\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  sns.scatterplot(data=df, x=\"experimental_ddg\", y=\"predicted_ddg\", alpha=0.5)\n",
        "  sns.lineplot(x=df[\"experimental_ddg\"], y=df[\"experimental_ddg\"], color=\"black\")\n",
        "  ax = plt.gca()\n",
        "  ax.set_ylabel(\"Predicted \\u0394\\u0394G\")\n",
        "  ax.set_xlabel(\"Experimental \\u0394\\u0394G\")\n",
        "  plt.tight_layout()\n",
        "  if title is not None:\n",
        "    ax.set_title(title)\n",
        "  if save_pth is not None:\n",
        "    plt.savefig(save_pth, dpi=300)\n",
        "    print(\"Successfully saved figure to:\", save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXj93QQXsWrc"
      },
      "outputs": [],
      "source": [
        "def get_boostrap_ci(y_true, y_pred, n:int, mode:str=\"rmse\"):\n",
        "  \"\"\"\n",
        "  Calculate 95% confidence interval using bootstrapping. Intended for use with RMSE and MAE\n",
        "  \"\"\"\n",
        "  np.random.seed(42)\n",
        "  values = np.empty(n)\n",
        "  for i in range(n):\n",
        "      indices = np.random.choice(len(y_true), len(y_true), replace=True)\n",
        "      y_true_sample = y_true[indices]\n",
        "      y_pred_sample = y_pred[indices]\n",
        "      if mode == \"rmse\":\n",
        "        values[i] = np.sqrt(mean_squared_error(y_true_sample, y_pred_sample))\n",
        "      elif mode == \"mae\":\n",
        "        values[i] = np.mean(np.abs(y_true_sample - y_pred_sample))\n",
        "      else:\n",
        "        raise NotImplementedError(\"mode expected to be 'rmse' or 'mae'\")\n",
        "  \n",
        "  # compute the 95% confidence interval\n",
        "  lower_b = np.percentile(values, 2.5)\n",
        "  upper_b = np.percentile(values, 97.5)\n",
        "  return lower_b, upper_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW3wM7lBL47c"
      },
      "outputs": [],
      "source": [
        "def print_metrics(df: pd.DataFrame, calculate_ci: bool=False):\n",
        "  \"\"\"\n",
        "  Print metrics from a given df\n",
        "  :param df: pd.DataFrame, expected to have cols 'experimental_ddg' and 'predicted_ddg'\n",
        "  :param calculate_ci: bool, whether to calculate confidence intervals\n",
        "  \"\"\"\n",
        "  corr_res = pearsonr(df[\"experimental_ddg\"], df[\"predicted_ddg\"])\n",
        "  mae = mean_absolute_error(df[\"experimental_ddg\"], df[\"predicted_ddg\"])\n",
        "  rmse =  mean_squared_error(df[\"experimental_ddg\"], df[\"predicted_ddg\"], squared=False)\n",
        "  print(\"Pearson correlation: %.4f\" % corr_res[0])\n",
        "  print(\"MAE: %.4f\" % mae)\n",
        "  print(\"RMSE: %.4f\" % rmse)\n",
        "  if calculate_ci:\n",
        "      corr_low, corr_high = corr_res.confidence_interval(confidence_level=0.95)\n",
        "      mae_ci = get_boostrap_ci(df[\"experimental_ddg\"],  df[\"predicted_ddg\"], len(df), mode=\"mae\")\n",
        "      rmse_ci = get_boostrap_ci(df[\"experimental_ddg\"], df[\"predicted_ddg\"], len(df), mode=\"rmse\")\n",
        "      print(\"Correlation interval: (%.4f, %.4f)\" % (corr_low, corr_high))\n",
        "      print(\"MAE interval: (%.4f, %.4f)\" % (mae_ci[0], mae_ci[1]))\n",
        "      print(\"RMSE interval: (%.4f, %.4f)\" % (rmse_ci[0], rmse_ci[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "wnZ1iewHbYrn",
        "outputId": "5432d4c0-dc87-4be7-c132-3583eccc9509"
      },
      "outputs": [],
      "source": [
        "print_metrics(ddgun_seq)\n",
        "create_parity_plot(ddgun_seq, os.path.join(FIG_SAVE_DIR, \"ddgun_seq.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "3LxsKV28g3gv",
        "outputId": "b72dd0d3-4b1c-4f34-d9cb-0028ad8f666e"
      },
      "outputs": [],
      "source": [
        "print_metrics(ddgun_alpha)\n",
        "create_parity_plot(ddgun_alpha, os.path.join(FIG_SAVE_DIR, \"ddgun_alpha_wild.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "-Pe-wt8ag8qJ",
        "outputId": "7c057499-0985-4299-d9b9-2da499745882"
      },
      "outputs": [],
      "source": [
        "print_metrics(ddgun_esm)\n",
        "create_parity_plot(ddgun_esm, os.path.join(FIG_SAVE_DIR, \"ddgun_esm_wild.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "brOc02ONbZLy",
        "outputId": "b46dc65a-519d-4f1d-e586-7dc531256d4d"
      },
      "outputs": [],
      "source": [
        "print_metrics(acdcnn_seq)\n",
        "create_parity_plot(acdcnn_seq, os.path.join(FIG_SAVE_DIR, \"acdcnn_seq.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "CYmDNuBk81kN",
        "outputId": "2c891f3c-9eda-4383-d4b5-762a720dc2fd"
      },
      "outputs": [],
      "source": [
        "print_metrics(acdcnn_alpha)\n",
        "create_parity_plot(acdcnn_alpha, os.path.join(FIG_SAVE_DIR, \"acdcnn_alpha_wild.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "Yr70WKF09ChT",
        "outputId": "18a58da3-adef-4487-c431-ee4732b0b8a9"
      },
      "outputs": [],
      "source": [
        "print_metrics(acdcnn_esm)\n",
        "create_parity_plot(acdcnn_esm, os.path.join(FIG_SAVE_DIR, \"acdcnn_esm_wild.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "e9L2iuwJhAY2",
        "outputId": "86ad358c-0599-4a3f-b5a2-f12c7a345693"
      },
      "outputs": [],
      "source": [
        "print_metrics(acdcnn_esm_mut)\n",
        "create_parity_plot(acdcnn_esm_mut, os.path.join(FIG_SAVE_DIR, \"acdcnn_esm_mut.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi34qybyTr4h"
      },
      "source": [
        "### Correlation between ddg prediction errors and plddt values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOYGHVsbdUV1"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "\n",
        "save_paths = [os.path.join(SAVE_DIR, \"ddgun_alphafold.csv\"), os.path.join(SAVE_DIR, \"ddgun_esmfold.csv\"),  \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_alphafold.csv\"), os.path.join(SAVE_DIR, \"acdcnn_esmfold.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"acdcnn_esmfold_muts.csv\"), os.path.join(SAVE_DIR, \"alphafold_wild_plddts.csv\"),\n",
        "              os.path.join(SAVE_DIR, \"esmfold_wild_plddts.csv\"), os.path.join(SAVE_DIR, \"esmfold_mutant_plddts.csv\"),]\n",
        "\n",
        "ddgun_alpha = pd.read_csv(save_paths[0])\n",
        "ddgun_esm = pd.read_csv(save_paths[1])\n",
        "acdcnn_alpha = pd.read_csv(save_paths[2])\n",
        "acdcnn_esm = pd.read_csv(save_paths[3])\n",
        "acdcnn_esm_mut = pd.read_csv(save_paths[4])\n",
        "alpha_plddts = pd.read_csv(save_paths[5])\n",
        "esm_wild_plddts = pd.read_csv(save_paths[6])\n",
        "esm_mut_plddts = pd.read_csv(save_paths[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQC5peiF84x2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calc_avg_rmses(df: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Calculate the average RMSE (root-mean square error) in predictions for each uniprot id.\n",
        "  :param df: pd.DataFrame containing uniprot ids, experimental ddgs, and predicted ddgs\n",
        "  :return: pd.DataFrame mapping uniprot id to average RMSE\n",
        "  \"\"\"\n",
        "  def calc_error(group):\n",
        "    return mean_squared_error(group[\"experimental_ddg\"], group[\"predicted_ddg\"], squared=False)\n",
        "  errors = df.groupby(\"uniprot_id\").apply(calc_error)\n",
        "  error_df = pd.DataFrame({\"uniprot_id\": errors.index, \"error\": errors.values})\n",
        "  return error_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwB21sxBGYiV"
      },
      "outputs": [],
      "source": [
        "def calc_rmses_mut(df: pd.DataFrame):\n",
        "  \"\"\"\n",
        "  Calculate the average RMSE (root-mean square error) in predictions for each mutation.\n",
        "  In this case RMSE is being calculated for each mutation so RMSE = |experimental - predicted| in this case\n",
        "  :param df: pd.DataFrame containing uniprot ids, experimental ddgs, and predicted ddgs\n",
        "  :return: pd.DataFrame mapping uniprot id and mutation to average RMSE\n",
        "  \"\"\"\n",
        "  error_df = df.copy()\n",
        "  def calc_error(entry):\n",
        "    return mean_squared_error([entry[\"experimental_ddg\"]], [entry[\"predicted_ddg\"]], squared=False)\n",
        "  errors = df.apply(calc_error, axis=1)\n",
        "  error_df[\"error\"] = errors\n",
        "  return error_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCzOCL0eDScc"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "def get_corr(error_df: pd.DataFrame, plddt_df: pd.DataFrame, merge_on):\n",
        "  \"\"\"\n",
        "  Get correlation (and correlation p-value) between prediction errors and structure plddts\n",
        "  \"\"\"\n",
        "  merged_df = pd.merge(error_df, plddt_df, on=merge_on)\n",
        "  correlation, p_value = pearsonr(merged_df[\"error\"], merged_df[\"avg_plddt\"])\n",
        "  return correlation, p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eumiqk8qE9vL"
      },
      "outputs": [],
      "source": [
        "ddgun_alpha_corr, ddgun_alpha_pval = get_corr(calc_avg_rmses(ddgun_alpha), alpha_plddts, merge_on=\"uniprot_id\")\n",
        "ddgun_esm_corr, ddgun_esm_pval = get_corr(calc_avg_rmses(ddgun_esm), esm_wild_plddts, merge_on=\"uniprot_id\")\n",
        "acdcnn_alpha_corr, acdcnn_alpha_pval = get_corr(calc_avg_rmses(acdcnn_alpha), alpha_plddts, merge_on=\"uniprot_id\")\n",
        "acdcnn_esm_corr, acdcnn_esm_pval = get_corr(calc_avg_rmses(acdcnn_esm), esm_wild_plddts, merge_on=\"uniprot_id\")\n",
        "acdcnn_esm_mut_coor, acdcnn_esm_mut_pval = get_corr(calc_rmses_mut(acdcnn_esm_mut), esm_mut_plddts, merge_on=[\"uniprot_id\", \"mut_code\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j6k0hoe1sCgh",
        "outputId": "fcf237ce-1952-437f-887b-48349fe12b7d"
      },
      "outputs": [],
      "source": [
        "corrs = [ddgun_alpha_corr, ddgun_esm_corr, acdcnn_alpha_corr, acdcnn_esm_corr, acdcnn_esm_mut_coor]\n",
        "pvals = [ddgun_alpha_pval, ddgun_esm_pval, acdcnn_alpha_pval, acdcnn_esm_pval, acdcnn_esm_mut_pval]\n",
        "df = pd.DataFrame(zip(corrs, pvals), columns=[\"Correlation\", \"P-Value\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOY6ptke3-Q2"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyQJzuuFStv"
      },
      "source": [
        "### Data plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPnvAqxFVOD"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "comb_save_pth = os.path.join(SAVE_DIR, \"final_prot_df.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67YuSc9zvVOq",
        "outputId": "865c26a0-9ebc-4b25-e1ff-724c6878b27d"
      },
      "outputs": [],
      "source": [
        "print(\"Number wild-type proteins:\", len(set(prot_df[\"uniprot_id\"])))\n",
        "print(\"Number mutations:\", len(prot_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOroL-xP9nLt"
      },
      "outputs": [],
      "source": [
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "UF-_RhkyIrXH",
        "outputId": "a2a539be-53ee-456d-d05b-c7575b4fd763"
      },
      "outputs": [],
      "source": [
        "# create histogram of ddG vals\n",
        "print('min ddG:', min(prot_df['ddG']))\n",
        "print('max ddG:', max(prot_df['ddG']))\n",
        "fig = sns.histplot(data=prot_df, x='ddG')\n",
        "fig.set_xlabel(\"\\u0394\\u0394G Value\")\n",
        "fig.set_ylabel(\"Frequency\")\n",
        "fig.set_title(\"Experiments by \\u0394\\u0394G\")\n",
        "plt.savefig(os.path.join(FIG_DIR, \"ddg_experimental_hist.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "TZLVN7RlOFUH",
        "outputId": "bbf94ec1-52e7-4c79-e285-404fd59aa853"
      },
      "outputs": [],
      "source": [
        "# show names of most frequent protein entries\n",
        "prot_cnts = prot_df['protein_name'].value_counts().rename_axis('protein_name').reset_index(name='counts')\n",
        "n = 10\n",
        "top_n = prot_cnts[0:n]\n",
        "fig = sns.barplot(data=top_n, x='counts', y='protein_name', orient='h', color='lightseagreen', alpha=0.5)\n",
        "fig.set_xlabel(\"Frequency\")\n",
        "fig.set_ylabel(\"\")\n",
        "fig.set_title(\"Top %d Proteins by Entry\" % n)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, \"top_n_proteins.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "nEPuhnYGVgFa",
        "outputId": "d9f5669c-696b-4b5f-8912-818ea8857ac1"
      },
      "outputs": [],
      "source": [
        "# map of amino acid substitions\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from_acids = prot_df['wild_type']\n",
        "to_acids = prot_df['mutation']\n",
        "# data not actually a confusion matrix, but this works for visualization purposes\n",
        "categories = sorted(list(set(from_acids)))\n",
        "cf_matrix = confusion_matrix(from_acids, to_acids)\n",
        "fig = sns.heatmap(cf_matrix, annot=False, xticklabels=categories, yticklabels=categories)\n",
        "plt.ylabel('From acid')\n",
        "plt.xlabel('To acid')\n",
        "fig.set_title(\"Amino Acid Substitutions\")\n",
        "sz_factor = 2\n",
        "sns.set(rc={'figure.figsize':(11.7 * sz_factor,8.27 * sz_factor)})\n",
        "plt.savefig(os.path.join(FIG_DIR, \"substitution_heatmap.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "o1nUSxcv0JdX",
        "outputId": "79ad3d79-59d2-4172-c82d-649b9e31c7ae"
      },
      "outputs": [],
      "source": [
        "# bar plot of amino acid substitutions\n",
        "from_acids = prot_df['wild_type']\n",
        "to_acids = prot_df['mutation']\n",
        "from_acid_cnts = from_acids.value_counts().rename_axis('acid').reset_index(name='From amino acid')\n",
        "to_acid_cnts = to_acids.value_counts().rename_axis('acid').reset_index(name='To amino acid')\n",
        "cnt_df = from_acid_cnts.merge(to_acid_cnts, on='acid').sort_values(by='acid')\n",
        "cnt_df = pd.melt(cnt_df, id_vars=[\"acid\"])\n",
        "cnt_df = cnt_df.rename(columns={\"variable\": \"source\", \"value\":\"entries\"})\n",
        "fig = sns.barplot(data=cnt_df, x='acid', y=\"entries\", hue='source')\n",
        "fig.set_title(\"Number of Entries by Amino Acid Substitution\")\n",
        "fig.set_xlabel(\"Amino Acid\")\n",
        "fig.set_ylabel(\"Frequency\")\n",
        "plt.savefig(os.path.join(FIG_DIR, \"amino_acid_entries.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "8avDFLzTY7W6",
        "outputId": "e7efc337-a58e-4289-e5e4-a124ee0acca1"
      },
      "outputs": [],
      "source": [
        "# plots sequence lengths\n",
        "seq_lens = prot_df['sequence'].map(lambda x : len(x))\n",
        "# ignore large proteins (~500 are larger than 1000 residues)\n",
        "print('min length:', min(seq_lens))\n",
        "print('max length:', max(seq_lens))\n",
        "fig = sns.histplot(data=seq_lens)\n",
        "fig.set_title(\"Sequence Length of Entries\")\n",
        "fig.set_xlabel(\"Sequence Length\")\n",
        "fig.set_ylabel(\"Frequency\")\n",
        "plt.savefig(os.path.join(FIG_DIR, \"seq_len_histogram.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "O2UomovuhOY1",
        "outputId": "01a99a0d-e954-49bc-873a-79826d0287a9"
      },
      "outputs": [],
      "source": [
        "# thanks to ChatGPT for helping write this \n",
        "# Create a new DataFrame with a column of unique strings and their counts\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# Convert the string entries to lists of strings\n",
        "families = prot_df['interpro_families'].apply(ast.literal_eval)\n",
        "\n",
        "counts = Counter([item for sublist in families for item in set(sublist)])\n",
        "\n",
        "# Create a DataFrame with the unique strings and their counts\n",
        "unique_values = pd.DataFrame(list(counts.items()), columns=['family', 'count'])\n",
        "\n",
        "# Create the histogram plot using seaborn\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x='family', y='count', data=unique_values)\n",
        "ax.set_title('Histogram of Unique Interpro Families')\n",
        "ax.set_xlabel('Family')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set(xticklabels=[])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVCXDuR5hN2h"
      },
      "source": [
        "### pLDDT Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKmKpbMTvGkW"
      },
      "outputs": [],
      "source": [
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsOdcwAQkBCZ"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "\n",
        "save_paths = [os.path.join(SAVE_DIR, \"alphafold_wild_plddts.csv\"), os.path.join(SAVE_DIR, \"esmfold_wild_plddts.csv\"), \n",
        "              os.path.join(SAVE_DIR, \"esmfold_mutant_plddts.csv\"),]\n",
        "\n",
        "alpha_plddts = pd.read_csv(save_paths[0])\n",
        "esm_wild_plddts = pd.read_csv(save_paths[1])\n",
        "esm_mut_plddts = pd.read_csv(save_paths[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKJoXtpyUQoE",
        "outputId": "6fab2730-37ca-4f1d-cf56-aceba1eec6fc"
      },
      "outputs": [],
      "source": [
        "print(\"AlphaFold Wild (Mean, Median): %.2f, %.2f\" % (alpha_plddts[\"avg_plddt\"].mean(), alpha_plddts[\"avg_plddt\"].median()))\n",
        "print(\"ESMFold Wild (Mean, Median):  %.2f, %.2f\" % (esm_wild_plddts[\"avg_plddt\"].mean(), esm_wild_plddts[\"avg_plddt\"].median()))\n",
        "print(\"ESMFold Mutants (Mean, Median):  %.2f, %.2f\" % (esm_mut_plddts[\"avg_plddt\"].mean(), esm_mut_plddts[\"avg_plddt\"].median()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_jarQ6hddZJ"
      },
      "outputs": [],
      "source": [
        "min_plddt = np.min([min(esm_wild_plddts[\"avg_plddt\"]), min(esm_mut_plddts[\"avg_plddt\"]), min(alpha_plddts[\"avg_plddt\"])])\n",
        "min_plddt = np.floor(min_plddt / 5) * 5 # floor to nearest multiple of 5\n",
        "bins = np.arange(min_plddt, 105, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "N8xBEkelis23",
        "outputId": "46d7937b-1227-4ca0-f11b-351158e36db8"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "# Plot the histograms on the subplots\n",
        "sns.histplot(ax=axs[0], data=alpha_plddts[\"avg_plddt\"], bins=bins)\n",
        "sns.histplot(ax=axs[1], data=esm_wild_plddts[\"avg_plddt\"], bins=bins)\n",
        "sns.histplot(ax=axs[2], data=esm_mut_plddts[\"avg_plddt\"], bins=bins)\n",
        "\n",
        "axs[0].set_ylabel(\"Frequency\")\n",
        "axs[1].set_ylabel(\"\")\n",
        "axs[2].set_ylabel(\"\")\n",
        "\n",
        "# Set the x-axis label for each subplot\n",
        "axs[0].set_xlabel(\"pLDDT Value\")\n",
        "axs[1].set_xlabel(\"pLDDT Value\")\n",
        "axs[2].set_xlabel(\"pLDDT Value\")\n",
        "\n",
        "# set title for each subplot\n",
        "axs[0].set_title(\"AlphaFold WildTypes\")\n",
        "axs[1].set_title(\"ESMFold WildTypes\")\n",
        "axs[2].set_title(\"ESMFold Mutants\")\n",
        "\n",
        "# show/save figure\n",
        "fig.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, \"plddt_histograms.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfzDg1x0Tp7t"
      },
      "source": [
        "### Structure visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7hFOnJVQzvA"
      },
      "outputs": [],
      "source": [
        "!pip install py3DMol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGXZ4bGV4Vke"
      },
      "outputs": [],
      "source": [
        "STRUCT_FIG_DIR = os.path.join(DATA_DIR, \"figures\", \"structures\")\n",
        "os.makedirs(STRUCT_FIG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ySqu-68hW7"
      },
      "outputs": [],
      "source": [
        "def load_pdb(pdb_path: str) -> str:\n",
        "  system = \"\"\n",
        "  with open(pdb_path) as ifile:\n",
        "    system = \"\".join([x for x in ifile])\n",
        "  return system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ0Mheg52_AM"
      },
      "outputs": [],
      "source": [
        "def visualize_pdb_with_confidence(pdb_path: str, save_path: str=None):\n",
        "  \"\"\"\n",
        "  Visualize a given pdb file, using plddt scores to color each atom.\n",
        "  \"\"\"\n",
        "  system = load_pdb(pdb_path)\n",
        "  view = py3Dmol.view(width=400, height=400)\n",
        "  view.addModelsAsFrames(system)\n",
        "\n",
        "  i = 0\n",
        "  for line in system.split(\"\\n\"):\n",
        "    split = line.split()\n",
        "    if len(split) == 0 or split[0] != \"ATOM\":\n",
        "      continue\n",
        "    plddt = float(split[-2])\n",
        "    if plddt >= 90:\n",
        "      color = \"#3434eb\"\n",
        "    elif plddt < 90 and plddt >= 70:\n",
        "      color = \"#34deeb\"\n",
        "    elif plddt < 70 and plddt >= 50:\n",
        "      color = \"#ebe534\"\n",
        "    else:\n",
        "      color = \"#eb7134\"\n",
        "    idx = int(split[1])\n",
        "    view.setStyle({'model': -1, 'serial': i+1}, {\"cartoon\": {'color': color}})\n",
        "    i += 1\n",
        "  view.zoomTo()\n",
        "  view.show()\n",
        "  view.render()\n",
        "  view.png()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KDdSywBY23S"
      },
      "outputs": [],
      "source": [
        "import py3Dmol\n",
        "# some py3DMol documentation available at:\n",
        "# https://william-dawson.github.io/using-py3dmol.html \n",
        "ALPHA_DIR = os.path.join(DATA_DIR, \"alphafold\", \"alphafold_preds\", \"wildtypes\")\n",
        "ESM_DIR = os.path.join(DATA_DIR, \"esmfold\", \"esm_preds\", \"wildtypes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eImkwgcBbzm"
      },
      "outputs": [],
      "source": [
        "uniprot_id = \"P04156\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u0lUlI03RAX"
      },
      "outputs": [],
      "source": [
        "pdb_path = os.path.join(ALPHA_DIR, uniprot_id + \".pdb\")\n",
        "save_path = os.path.join(STRUCT_FIG_DIR, uniprot_id +\".png\")\n",
        "visualize_pdb_with_confidence(pdb_path, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FKXLAWOBY35"
      },
      "outputs": [],
      "source": [
        "def find_pdb_path(dir: str, uni_id: str):\n",
        "  for subdir in os.listdir(dir):\n",
        "    if os.path.exists(os.path.join(dir, subdir, uni_id + \".pdb\")):\n",
        "      return os.path.join(dir, subdir, uni_id + \".pdb\")\n",
        "  return None\n",
        "pdb_path = find_pdb_path(ESM_DIR, uniprot_id)\n",
        "save_path = os.path.join(STRUCT_FIG_DIR, uniprot_id +\".png\")\n",
        "visualize_pdb_with_confidence(pdb_path, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwqp66i44Ai2"
      },
      "outputs": [],
      "source": [
        "# taken from:\n",
        "# https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb\n",
        "\n",
        "\n",
        "def plot_plddt_legend(save_pth: str):\n",
        "  \"\"\"Plots the legend for pLDDT.\"\"\"\n",
        "  thresh = ['Very high (pLDDT > 90)', \n",
        "            'Confident (90 > pLDDT > 70)',\n",
        "            'Low (70 > pLDDT > 50)',\n",
        "            'Very low (pLDDT < 50)']\n",
        "\n",
        "  colors = [\"#3434eb\", \"#34deeb\", \"#ebe534\", \"#eb7134\"]\n",
        "\n",
        "  plt.figure(figsize=(2, 2))\n",
        "  for c in colors:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  ax = plt.gca()\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['bottom'].set_visible(False)\n",
        "  plt.title('Model Confidence', fontsize=20, pad=20)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(save_pth, dpi=300)\n",
        "  return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "6iPfkEcSOlQk",
        "outputId": "1a58029a-5e11-46f8-dc8b-5dce945c1955"
      },
      "outputs": [],
      "source": [
        "plot_plddt_legend(os.path.join(STRUCT_FIG_DIR, \"legend.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slx-lsrvR3vt"
      },
      "source": [
        "# Misc/Unused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXmjUCFAj_8L"
      },
      "source": [
        "### Gathering Experimental PDB Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_mXisWNlmoq"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = os.path.join(DATA_DIR, \"cleaned_final_data\")\n",
        "comb_save_pth = os.path.join(SAVE_DIR, \"final_prot_df.csv\")\n",
        "prot_df = pd.read_csv(comb_save_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e0TwX3wj_MV"
      },
      "outputs": [],
      "source": [
        "# adapted from:\n",
        "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n",
        "\n",
        "def download_pdb(url: str, uniprot_id: str, save_pth: str):\n",
        "  if url is None:\n",
        "    return False\n",
        "  try:\n",
        "    with requests.get(url, stream=True) as r:\n",
        "      r.raise_for_status()\n",
        "      with open(save_pth, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192): \n",
        "          f.write(chunk)\n",
        "    return True\n",
        "  except requests.exceptions.HTTPError:\n",
        "    # protein not found in database\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfeVYvzo0Ktw"
      },
      "outputs": [],
      "source": [
        "# thanks to chatgpt for helping write this function\n",
        "def get_target_url(uniprot_id: str):\n",
        "  # Make a GET request to retrieve the mapping data for the UniProt ID\n",
        "  response = requests.get(f\"https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/{uniprot_id}\")\n",
        "  if not response.ok:\n",
        "    return None, \"\"\n",
        "  # Parse the JSON response\n",
        "  data = json.loads(response.text)\n",
        "\n",
        "  # Extract the PDB IDs and coverage/resolution data\n",
        "  pdb_coverage_data = []\n",
        "  for pdb_entry in data[uniprot_id]:\n",
        "      pdb_id = pdb_entry[\"pdb_id\"]\n",
        "      coverage = pdb_entry[\"coverage\"]\n",
        "      resolution = pdb_entry[\"resolution\"]\n",
        "      chain_id = pdb_entry['chain_id']\n",
        "      if resolution is None:\n",
        "        resolution = 10.0 # NMR methods don't provide resolution\n",
        "      if len(chain_id) > 1:\n",
        "        resolution = 100.0 # de-prioritize methods which have many chains\n",
        "      pdb_coverage_data.append((pdb_id, coverage, resolution, chain_id))\n",
        "\n",
        "  # Sort the PDB data by coverage and resolution\n",
        "  sorted_pdb_coverage_data = sorted(pdb_coverage_data, key=lambda x: (x[1], -x[2]), reverse=True)\n",
        "\n",
        "  # Get the PDB ID of the top-ranked structure\n",
        "  top_pdb_id = sorted_pdb_coverage_data[0][0]\n",
        "  chain_info = sorted_pdb_coverage_data[0][3]\n",
        "  return f\"https://files.rcsb.org/download/{top_pdb_id}.pdb\", chain_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ADWJeNGlDlJ",
        "outputId": "230e00a3-d8dd-460b-8ddd-315f08e3a788"
      },
      "outputs": [],
      "source": [
        "# setup directory to save predictions to\n",
        "EXPERIMENTAL_DIR = os.path.join(DATA_DIR, \"experimental_structures\", \"wildtypes\")\n",
        "os.makedirs(EXPERIMENTAL_DIR, exist_ok=True)\n",
        "\n",
        "# uniprot ids not found in database\n",
        "not_found = [] \n",
        "\n",
        "# save all wild-type experimental structures\n",
        "uniprot_ids = set(prot_df['uniprot_id'])\n",
        "investigate = []\n",
        "for uid in tqdm(uniprot_ids):\n",
        "  url_target, chain_info = get_target_url(uid)\n",
        "  if len(chain_info) > 1:\n",
        "    investigate.append((uid, url_target, chain_info))\n",
        "  fname = \"%s.pdb\" % uid\n",
        "  save_pth = os.path.join(EXPERIMENTAL_DIR, fname)\n",
        "  found = download_pdb(url_target, uid, save_pth)\n",
        "  if not(found):\n",
        "    not_found.append(uid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l511YzFJk-4o",
        "outputId": "66f33b08-9c14-41bf-bf92-565c04761993"
      },
      "outputs": [],
      "source": [
        "print(len(investigate))\n",
        "print(investigate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1LK0Exa2WJL",
        "outputId": "399ac8b9-32f9-427d-8ed1-c65edf7eb7e5"
      },
      "outputs": [],
      "source": [
        "print(not_found) # no experimental structures for these proteins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZCB4JeWt4Or"
      },
      "source": [
        "### Change to 'A' chain\n",
        "To facilate comparison with ESMFold/AlphaFold. Even though many entries only have one chain for the protein structure additional information is often included that makes external tools difficult to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZC0mF0bx_Gg"
      },
      "outputs": [],
      "source": [
        "# Open the input PDB file\n",
        "def to_a_chain(pdb_path: str, save_path: str):\n",
        "  with open(pdb_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  # Modify the header to set the chain identifier to \"A\"\n",
        "  for i, line in enumerate(lines):\n",
        "    if line.startswith('ATOM'):\n",
        "      lines[i] = line[:21] + 'A' + line[22:]\n",
        "\n",
        "  # Write the modified PDB file to disk\n",
        "  with open(save_path, 'w') as f:\n",
        "    f.writelines(lines) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYHJtd06ylSi",
        "outputId": "d1c30546-7a28-4920-a629-36de49ec8ced"
      },
      "outputs": [],
      "source": [
        "EXPERIMENTAL_DIR = os.path.join(DATA_DIR, \"experimental_structures\", \"wildtypes\")\n",
        "EXPERIMENTAL_A_DIR = os.path.join(DATA_DIR, \"experimental_structures\", \"wildtypes_A_chain\")\n",
        "os.makedirs(EXPERIMENTAL_A_DIR, exist_ok=True)\n",
        "\n",
        "for pdb_file in tqdm(os.listdir(EXPERIMENTAL_DIR)):\n",
        "  uid = pdb_file[:pdb_file.index(\".pdb\")]\n",
        "  pdb_path = os.path.join(EXPERIMENTAL_DIR, pdb_file)\n",
        "  save_path = os.path.join(EXPERIMENTAL_A_DIR, f\"{uid}.pdb\")\n",
        "  to_a_chain(pdb_path, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NidAHnWbtPXX"
      },
      "outputs": [],
      "source": [
        "# print chains from downloaded pdbs\n",
        "from Bio import PDB\n",
        "\n",
        "EXPERIMENTAL_DIR = os.path.join(DATA_DIR, \"experimental_structures\", \"wildtypes_A_chain\")\n",
        "\n",
        "ls = []\n",
        "for pdb_file in os.listdir(EXPERIMENTAL_DIR)[1:2]:\n",
        "  # Create a PDB parser object\n",
        "  parser = PDB.PDBParser()\n",
        "  pdb_path = os.path.join(EXPERIMENTAL_DIR, pdb_file)\n",
        "  print(pdb_path)\n",
        "  # Parse the PDB file\n",
        "  structure = parser.get_structure('my_structure', pdb_path)\n",
        "\n",
        "  # Get the chain(s) in the structure\n",
        "  chains = [chain.get_id() for chain in structure.get_chains()]\n",
        "  for ch in chains:\n",
        "    ls.append(ch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2yGwGdp0cZ1",
        "outputId": "cfa9d2a6-25bb-4895-e644-bad47c32227b"
      },
      "outputs": [],
      "source": [
        "print(set(ls))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWftbOARo-Cn"
      },
      "source": [
        "### Calculate RMSD between two pdb files\n",
        "The root-mean-square deviation (RMSD), is the measure of the average distance between the atoms (usually the backbone atoms) of superimposed protein.\n",
        "See: https://en.wikipedia.org/wiki/Root-mean-square_deviation_of_atomic_positions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5IaFvREEaFC",
        "outputId": "00c501da-134f-46f2-b9c4-9c7700159a90"
      },
      "outputs": [],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byVRnJDsElQ4"
      },
      "outputs": [],
      "source": [
        "# adapted from: https://github.com/sarisabban/RMSD/blob/main/RMSD.py \n",
        "import Bio.PDB\n",
        "# Bio.PDB citation: \n",
        "# Hamelryck, T., Manderick, B. (2003) PDB parser and structure class implemented in Python. Bioinformatics 19: 23082310\n",
        "\n",
        "\n",
        "def get_rmsd(pdb_path1: str, pdb_path2: str):\n",
        "  '''\n",
        "  Calculate the RMSD between two protein structures using Biopython\n",
        "  The Biopython algorithm is poorly designed and only aligns local motifs\n",
        "  rather than full protein structures/complexes.\n",
        "  '''\n",
        "  builder = Bio.PDB.Polypeptide.PPBuilder()\n",
        "  parser = Bio.PDB.PDBParser(QUIET=False)\n",
        "  struct1 = parser.get_structure('Structure 1', pdb_path1)\n",
        "  struct2 = parser.get_structure('Structure 2', pdb_path2)\n",
        "  fixed = [atom for atom in struct1.get_atoms()]\n",
        "  moving = [atom for atom in struct2.get_atoms()]\n",
        "  lengths = [len(fixed), len(moving)]\n",
        "  smallest = min(lengths)\n",
        "  sup = Bio.PDB.Superimposer()\n",
        "  sup.set_atoms(fixed[:smallest], moving[:smallest])\n",
        "  return sup.rms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYsdTosA8HXQ",
        "outputId": "ba4c35f3-46a2-4714-9635-56f0879dd40e"
      },
      "outputs": [],
      "source": [
        "uniprot_id = 'P37957'\n",
        "pdb_esm_dir = os.path.join(DATA_DIR, \"pdbs/wild_type/esm\")\n",
        "pdb_esm_path = os.path.join(pdb_esm_dir, \"%s.pdb\" % uniprot_id)\n",
        "pdb_alpha_dir = os.path.join(DATA_DIR, \"pdbs/wild_type/alphafold\")\n",
        "pdb_alpha_path = os.path.join(pdb_alpha_dir, '%s.pdb' % uniprot_id)\n",
        "\n",
        "rmsd = get_rmsd(pdb_esm_path, pdb_alpha_path)\n",
        "print(rmsd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HFmiiap05Z5b",
        "dSgzzHswFbXC",
        "9nQYroa15mMc",
        "vkCfpM4Nb6xP",
        "lS_zTo5I90cJ",
        "xxyQJzuuFStv"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
